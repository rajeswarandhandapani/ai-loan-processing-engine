"""
Document analysis cache utility.

Provides file-based caching for Azure Document Intelligence API results
to reduce API calls and improve performance during development and testing.
"""

import hashlib
import json
from pathlib import Path
from typing import Optional, Any
from pydantic import BaseModel

from app.config import settings, BACKEND_DIR
from app.logging_config import get_logger

logger = get_logger(__name__)


class DocumentCache:
    """File-based cache for document analysis results."""
    
    def __init__(self, cache_dir: Optional[Path] = None, enabled: Optional[bool] = None):
        """
        Initialize the document cache.
        
        Args:
            cache_dir: Directory to store cache files (defaults to settings)
            enabled: Whether caching is enabled (defaults to settings)
        """
        self.enabled = enabled if enabled is not None else settings.DOCUMENT_CACHE_ENABLED
        self.cache_dir = cache_dir or (BACKEND_DIR / settings.DOCUMENT_CACHE_DIR)
        
        if self.enabled:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Document cache enabled at: {self.cache_dir}")
        else:
            logger.info("Document cache disabled")
    
    def get_cache_key(self, file_path: Path, document_type: str) -> str:
        """
        Generate a unique cache key based on file content and document type.
        
        Args:
            file_path: Path to the document file
            document_type: Type of document being analyzed
            
        Returns:
            Unique cache key string
        """
        with open(file_path, "rb") as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return f"{file_hash}_{document_type}"
    
    def get_cache_path(self, cache_key: str) -> Path:
        """
        Get the cache file path for a given cache key.
        
        Args:
            cache_key: Cache key generated by get_cache_key()
            
        Returns:
            Path to the cache file
        """
        return self.cache_dir / f"{cache_key}.json"
    
    def load(self, cache_key: str, model_class: type[BaseModel]) -> Optional[BaseModel]:
        """
        Load cached result from disk.
        
        Args:
            cache_key: Cache key to load
            model_class: Pydantic model class to deserialize into
            
        Returns:
            Cached model instance or None if not found/invalid
        """
        if not self.enabled:
            return None
        
        cache_path = self.get_cache_path(cache_key)
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, "r") as f:
                cached_data = json.load(f)
            logger.info(f"✓ Cache HIT: Loaded from {cache_path.name}")
            return model_class(**cached_data)
        except Exception as e:
            logger.warning(f"Failed to load cache {cache_path.name}: {e}")
            return None
    
    def save(self, cache_key: str, data: BaseModel) -> bool:
        """
        Save result to cache.
        
        Args:
            cache_key: Cache key to save under
            data: Pydantic model instance to cache
            
        Returns:
            True if saved successfully, False otherwise
        """
        if not self.enabled:
            return False
        
        cache_path = self.get_cache_path(cache_key)
        try:
            with open(cache_path, "w") as f:
                json.dump(data.model_dump(), f, indent=2, default=str)
            logger.info(f"✓ Cache SAVED: {cache_path.name}")
            return True
        except Exception as e:
            logger.warning(f"Failed to save cache {cache_path.name}: {e}")
            return False
    
    def clear(self, cache_key: Optional[str] = None) -> int:
        """
        Clear cache entries.
        
        Args:
            cache_key: Specific cache key to clear, or None to clear all
            
        Returns:
            Number of cache files deleted
        """
        if not self.enabled:
            return 0
        
        count = 0
        if cache_key:
            # Clear specific cache entry
            cache_path = self.get_cache_path(cache_key)
            if cache_path.exists():
                cache_path.unlink()
                count = 1
                logger.info(f"Cleared cache: {cache_path.name}")
        else:
            # Clear all cache entries
            for cache_file in self.cache_dir.glob("*.json"):
                cache_file.unlink()
                count += 1
            logger.info(f"Cleared {count} cache entries")
        
        return count
    
    def get_stats(self) -> dict[str, Any]:
        """
        Get cache statistics.
        
        Returns:
            Dictionary with cache stats (enabled, directory, file count, total size)
        """
        stats = {
            "enabled": self.enabled,
            "cache_dir": str(self.cache_dir),
            "file_count": 0,
            "total_size_bytes": 0,
        }
        
        if self.enabled and self.cache_dir.exists():
            cache_files = list(self.cache_dir.glob("*.json"))
            stats["file_count"] = len(cache_files)
            stats["total_size_bytes"] = sum(f.stat().st_size for f in cache_files)
            stats["total_size_mb"] = round(stats["total_size_bytes"] / (1024 * 1024), 2)
        
        return stats
